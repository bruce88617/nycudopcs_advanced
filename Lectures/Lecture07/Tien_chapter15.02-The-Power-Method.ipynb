{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "button": false,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "source": [
    "<!--NAVIGATION-->\n",
    "< [5.1 Eigenvalues and Eignevectors](Tien_chapter15.01-Eigenvalues-and-Eigenvectors-Problem-Statement.ipynb) | [Contents](Tien_chapter15.02-The-Power-Method.ipynb) | [5.3 The QR Method](Tien_chapter15.03-The-QR-Method.ipynb)  >"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "button": false,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "source": [
    "### **5.2.1 The Power Method (Iteration)**"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "button": false,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "source": [
    "#### <font color=\"cyan\">**Find the largest eigenvalue**</font>\n",
    "\n",
    "In some problems, we only need to find the largest dominant eigenvalue and its corresponding eigenvector. In this case, we can use the power method - a iterative method that will converge to the largest eigenvalue. Let's see the following how the power method works. \n",
    "\n",
    "Consider an $n\\times{n}$ matrix $A$ that has $n$ linearly independent real eigenvalues $\\lambda_1, \\lambda_2, \\dots, \\lambda_n$ and the corresponding eigenvectors $v_1, v_2, \\dots, v_n$. Since the eigenvalues are scalars, we can rank them so that $|\\lambda_1| > |\\lambda_2| > \\dots > |\\lambda_n| $ (actually, we only require $|\\lambda_1| > |\\lambda_2|$, other eigenvalues may be equal to each other). \n",
    "\n",
    "Because the eigenvectors are independent, they are a set of basis vectors, which means that any vector that is in the same space can be written as a linear combination of the basis vectors. That is, for any vector $x_0$, it can be written as:\n",
    "\n",
    "$$ x_0 = c_1v_1+c_2v_2+\\dots+c_nv_n$$\n",
    "\n",
    "**where $c_1\\ne0$ is the constraint.** If it is zero, then we need to choose another initial vector so that $c_1\\ne0$. \n",
    "\n",
    "Now let's multiply both sides by $A$:\n",
    "\n",
    "$$ Ax_0 = c_1Av_1+c_2Av_2+\\dots+c_nAv_n$$\n",
    "\n",
    "Since $Av_i = \\lambda{v_i}$, we will have:\n",
    "\n",
    "$$ Ax_0 = c_1\\lambda_1v_1+c_2\\lambda_2v_2+\\dots+c_n\\lambda_nv_n$$\n",
    "\n",
    "We can change the above equation to:\n",
    "\n",
    "$$ Ax_0 = c_1\\lambda_1[v_1+\\frac{c_2}{c_1}\\frac{\\lambda_2}{\\lambda_1}v_2+\\dots+\\frac{c_n}{c_1}\\frac{\\lambda_n}{\\lambda_1}v_n]= c_1\\lambda_1x_1$$ \n",
    "\n",
    "**where $x_1$ is a new vector and $x_1 = v_1+\\frac{c_2}{c_1}\\frac{\\lambda_2}{\\lambda_1}v_2+\\dots+\\frac{c_n}{c_1}\\frac{\\lambda_n}{\\lambda_1}v_n$.**\n",
    "\n",
    "This finishes the first iteration. And we can multiply $A$ to $x_1$ to start the 2nd iteration:\n",
    "\n",
    "$$ Ax_1 = \\lambda_1{v_1}+\\frac{c_2}{c_1}\\frac{\\lambda_2^2}{\\lambda_1}v_2+\\dots+\\frac{c_n}{c_1}\\frac{\\lambda_n^2}{\\lambda_1}v_n $$ \n",
    "\n",
    "Similarly, we can rearrange the above equation to:\n",
    "\n",
    "$$ Ax_1 = \\lambda_1[v_1+\\frac{c_2}{c_1}\\frac{\\lambda_2^2}{\\lambda_1^2}v_2+\\dots+\\frac{c_n}{c_1}\\frac{\\lambda_n^2}{\\lambda_1^2}v_n] = \\lambda_1x_2$$ \n",
    "\n",
    "**where $x_2$ is another new vector and $x_2 = v_1+\\frac{c_2}{c_1}\\frac{\\lambda_2^2}{\\lambda_1^2}v_2+\\dots+\\frac{c_n}{c_1}\\frac{\\lambda_n^2}{\\lambda_1^2}v_n$.**\n",
    "\n",
    "We can continue multiply $A$ with the new vector we get from each iteration $k$ times:\n",
    "\n",
    "$$ Ax_{k-1} = \\lambda_1[v_1+\\frac{c_2}{c_1}\\frac{\\lambda_2^k}{\\lambda_1^k}v_2+\\dots+\\frac{c_n}{c_1}\\frac{\\lambda_n^k}{\\lambda_1^k}v_n] = \\lambda_1x_k$$ \n",
    "\n",
    "Because $\\lambda_1$ is the largest eigenvalue, therefore, the ratio $\\frac{\\lambda_i}{\\lambda_1}<1$ for all $i>1$. Thus when we increase $k$ to sufficient large, the ratio of $(\\frac{\\lambda_n}{\\lambda_1})^{k}$ will be close to 0. So that all the terms that contain this ratio can be neglected as $k$ grows:\n",
    "\n",
    "$$ Ax_{k-1} = {\\lambda_1}v_1 $$\n",
    "\n",
    "<font color=\"cyan\"> Essentially, as k is large enough, we will get the largest eigenvalue and its corresponding eigenvector. </font> \n",
    "\n",
    "When implementing this power method, we usually <font color= \"cyan\">normalize the resulting vector in each iteration</font>. This can be done by factoring out the largest element in the vector, which will make the largest element in the vector equal to 1. This normalization will get us the largest eigenvalue and its corresponding eigenvector at the same time. Let's take a look of the following example. "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When to stop the iteration? <br> \n",
    "1. the difference between eigenvalues is less than some specified tolerance; <br>\n",
    "2. the angle between eigenvectors is smaller than a threshold ; <br>\n",
    "3. the norm of the residual vector is small enough. "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color=\"magenta\">**TRY IT!**</font> We know from last section that the largest eigenvalue $\\lambda_1 = 4$  for matrix $A = \\begin{bmatrix}\n",
    "0 & 2\\\\\n",
    "2 & 3\\\\\n",
    "\\end{bmatrix}$, now use the power method to find the largest eigenvalue and the associated eigenvector. You can use the initial vector $x_0 = [1, 1]$ to start the iteration. "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "button": false,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "source": [
    "1st iteration: \n",
    "$$\n",
    "\\begin{bmatrix}\n",
    "0 & 2\\\\\n",
    "2 & 3\\\\\n",
    "\\end{bmatrix}\n",
    "\\begin{bmatrix}\n",
    "1\\\\1\\\\\n",
    "\\end{bmatrix}\n",
    "=\\begin{bmatrix}\n",
    "2\\\\5\\\\\n",
    "\\end{bmatrix}\n",
    "=5\\begin{bmatrix}\n",
    "0.4\\\\1\\\\\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "2nd iteration:\n",
    "$$\n",
    "\\begin{bmatrix}\n",
    "0 & 2\\\\\n",
    "2 & 3\\\\\n",
    "\\end{bmatrix}\n",
    "\\begin{bmatrix}\n",
    "0.4\\\\1\\\\\n",
    "\\end{bmatrix}\n",
    "=\\begin{bmatrix}\n",
    "2\\\\3.8\\\\\n",
    "\\end{bmatrix}\n",
    "=3.8\\begin{bmatrix}\n",
    "0.5263\\\\1\\\\\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "3rd iteration:\n",
    "$$\n",
    "\\begin{bmatrix}\n",
    "0 & 2\\\\\n",
    "2 & 3\\\\\n",
    "\\end{bmatrix}\n",
    "\\begin{bmatrix}\n",
    "0.5263\\\\1\\\\\n",
    "\\end{bmatrix}\n",
    "=\\begin{bmatrix}\n",
    "2\\\\ 4.0526\\\\\n",
    "\\end{bmatrix}\n",
    "= 4.0526\\begin{bmatrix}\n",
    "0.4935\\\\1\\\\\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "4th iteration:\n",
    "$$\n",
    "\\begin{bmatrix}\n",
    "0 & 2\\\\\n",
    "2 & 3\\\\\n",
    "\\end{bmatrix}\n",
    "\\begin{bmatrix}\n",
    "0.4935\\\\1\\\\\n",
    "\\end{bmatrix}\n",
    "=\\begin{bmatrix}\n",
    "2\\\\ 3.987\\\\\n",
    "\\end{bmatrix}\n",
    "= 3.987\\begin{bmatrix}\n",
    "0.5016\\\\1\\\\\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "5th iteration:\n",
    "$$\n",
    "\\begin{bmatrix}\n",
    "0 & 2\\\\\n",
    "2 & 3\\\\\n",
    "\\end{bmatrix}\n",
    "\\begin{bmatrix}\n",
    "0.5016\\\\1\\\\\n",
    "\\end{bmatrix}\n",
    "=\\begin{bmatrix}\n",
    "2\\\\ 4.0032\\\\\n",
    "\\end{bmatrix}\n",
    "= 4.0032\\begin{bmatrix}\n",
    "0.4996\\\\1\\\\\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "6th iteration:\n",
    "$$\n",
    "\\begin{bmatrix}\n",
    "0 & 2\\\\\n",
    "2 & 3\\\\\n",
    "\\end{bmatrix}\n",
    "\\begin{bmatrix}\n",
    "0.4996\\\\1\\\\\n",
    "\\end{bmatrix}\n",
    "=\\begin{bmatrix}\n",
    "2\\\\ 3.9992\\\\\n",
    "\\end{bmatrix}\n",
    "= 3.9992\\begin{bmatrix}\n",
    "0.5001\\\\1\\\\\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "7th iteration:\n",
    "$$\n",
    "\\begin{bmatrix}\n",
    "0 & 2\\\\\n",
    "2 & 3\\\\\n",
    "\\end{bmatrix}\n",
    "\\begin{bmatrix}\n",
    "0.5001\\\\1\\\\\n",
    "\\end{bmatrix}\n",
    "=\\begin{bmatrix}\n",
    "2\\\\ 4.0002\\\\\n",
    "\\end{bmatrix}\n",
    "= 4.0002\\begin{bmatrix}\n",
    "0.5000\\\\1\\\\\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "We can see after 7 iterations, the eigenvalue converged to 4 with [0.5, 1] as the corresponding eigenvector. \n",
    "\n",
    "**TRY IT!** Implement the power method in Python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "button": false,
    "collapsed": true,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Example 5.2: Maximum Eigenvealues and Eigenvectors by Power Method A = [[0, 2], [2, 3]]\n",
    "Eigenvealues        4,       \n",
    "Eigenvectors [0.5, 1], [    ]\n",
    "As iteration increases, the transformed vector will approach to largest eigenvectors\n",
    "\"\"\"\n",
    "import numpy as np \n",
    "\n",
    "def normalize(x):\n",
    "    i = np.argmax(abs(x))   # consider negative magnitude\n",
    "    fac = x[i]                 \n",
    "    x_n = x / fac\n",
    "    return fac, x_n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Eigenvalue: 3.999949137887188\n",
      "Eigenvector: [0.50000636 1.        ]\n"
     ]
    }
   ],
   "source": [
    "x = np.array([1, 1])\n",
    "A = np.array([[0, 2], [2, 3]])\n",
    "\n",
    "# fixed iteration number \n",
    "for i in range(8):\n",
    "    x = np.dot(A, x)\n",
    "    lambda_1, x = normalize(x)\n",
    "\n",
    "print(\"Eigenvalue:\", lambda_1)\n",
    "print(\"Eigenvector:\", x)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "button": false,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "source": [
    "### **5.2.2 The Inverse Power Method (Iteration)**\n",
    "\n",
    "The eigenvalues of the inverse matrix $A^{-1}$ are the reciprocals of the eigenvalues of $A$. We can take advantage of this feature as well as the power method to get the smallest eigenvalue of $A$, this will be basis of the inverse power method. The steps are very simple, instead of multiplying $A$ as described above, we just multiply $A^{-1}$ for our iteration to find the largest value of $\\frac{1}{\\lambda_1}$, which will be the smallest value of the eigenvalues for $A$. As for the inverse of the matrix, in practice, we can use the methods we covered in the previous chapter to calculate it. We won't got to the details here, but let's see an example. \n",
    "\n",
    "<font color=\"magenta\">**TRY IT!**</font> Find the <font color=\"cyan\">smallest</font> eigenvalue and eigenvector for $A = \\begin{bmatrix}\n",
    "0 & 2\\\\\n",
    "2 & 3\\\\\n",
    "\\end{bmatrix}$ through $A^{-1} = \\begin{bmatrix}\n",
    "-0.75 & 0.5\\\\\n",
    " 0.5  &   0\\\\\n",
    "\\end{bmatrix}$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A_inv= \n",
      "[[-0.75  0.5 ]\n",
      " [ 0.5   0.  ]]\n",
      "\n",
      " k,    lambda_1,       x\n",
      " 0,   0.50,  [-0.5, 1.0]\n",
      " 1,   0.88,  [1.0, -0.2857142857142857]\n",
      " 2,  -0.89,  [1.0, -0.56]\n",
      " 3,  -1.03,  [1.0, -0.4854368932038835]\n",
      " 4,  -0.99,  [1.0, -0.5036674816625917]\n",
      " 5,  -1.00,  [1.0, -0.49908480780964004]\n",
      " 6,  -1.00,  [1.0, -0.5002289027926141]\n",
      " 7,  -1.00,  [1.0, -0.4999427808506581]\n",
      "Eigenvalue: -1.000114451396307\n",
      "Eigenvector: [ 1.         -0.49994278]\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Example 5.3: Eigenvealues and Eigenvectors by Power Method A^-1 = [[-0.75, -0.5], [0.5, 0]]\n",
    "Eigenvealues       -1,         \n",
    "Eigenvectors [1, -0.5], [    ]\n",
    "As iteration increases, the transformed vector will approach to largest eigenvectors\n",
    "\"\"\"\n",
    "import numpy as np\n",
    "from numpy.linalg import inv\n",
    "\n",
    "def normalize(x):\n",
    "    i = np.argmax(abs(x))     # consider negative magnitude\n",
    "    fac = x[i]     \n",
    "    x_n = x / fac\n",
    "    return fac, x_n\n",
    "\n",
    "\n",
    "x = np.array([1, 1])\n",
    "A = np.array([[0, 2], [2, 3]])\n",
    "A_inv = np.linalg.inv(A)\n",
    "print(f\"A_inv= \\n{A_inv}\\n\")\n",
    "\n",
    "print(' k,    lambda_1,       x')\n",
    "iter_number = 8\n",
    "\n",
    "for i in range(iter_number):\n",
    "    x = np.dot(A_inv, x)\n",
    "    lambda_1, x = normalize(x)\n",
    "    print(f\"{i:2d},  {lambda_1: 2.2f},  {list(x)}\")\n",
    "\n",
    "print('Eigenvalue:', lambda_1)\n",
    "print('Eigenvector:', x)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "button": false,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "source": [
    "### **5.2.3 The Shifted Power Method (Iteration)**\n",
    "\n",
    "In some cases, we need to find all the eigenvalues and eigenvectors instead of the largest and smallest. One simple but inefficient way is to use the shifted power method (we will introduce you an efficient way in next section). Given $Ax = \\lambda{x}$, and $\\lambda_1$ is the largest eigenvalue obtained by the power method, then we can have:\n",
    "\n",
    "$$[A - \\lambda_1I]x = \\alpha{x}$$\n",
    "\n",
    "where $\\alpha$'s are the eigenvalues of the shifted matrix $A - \\lambda_1I$, which will be $0, \\lambda_2-\\lambda_1, \\lambda_3-\\lambda_1, \\dots, \\lambda_n-\\lambda_1$. \n",
    "\n",
    "Now if we apply the power method to the shifted matrix, then we can determine the largest eigenvalue of the shifted matrix, i.e. $\\alpha_k$. Since $\\alpha_k = \\lambda_k - \\lambda_1$, we can get the eigenvalue $\\lambda_k$ easily. We can repeat this process many times to find the all the other eigenvalues. But you can see that, it involves a lot of work! A better method for finding all the eigenvalues is to use the QR method, let's see the next section how it works! "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "button": false,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "source": [
    "<!--NAVIGATION-->\n",
    "< [5.1 Eigenvalues and Eignevectors](Tien_chapter15.01-Eigenvalues-and-Eigenvectors-Problem-Statement.ipynb) | [Contents](Tien_chapter15.02-The-Power-Method.ipynb) | [5.3 The QR Method](Tien_chapter15.03-The-QR-Method.ipynb)  >"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "numerical_methods",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  },
  "vscode": {
   "interpreter": {
    "hash": "a7632bf4336177dcaac0f16a29bf528d1eef8e973ee46e49b304d96d25ec4172"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
