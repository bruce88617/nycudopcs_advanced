{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center><h1><b>Lecture 4</b></h1></center>\n",
    "<center><h1><b>Introduction to Statistical Inference</b></h1></center>\n",
    "\n",
    "<center><h4>Time: T2 (09:00 ~ 09:50) and W34 (10:10 ~ 12:00)</h4></center>\n",
    "\n",
    "<center><h2>Chieh-En Lee<sup>1</sup> (李杰恩) and Chung-Hao Tien<sup>2</sup> (田仲豪)</h2></center>\n",
    "\n",
    "<center>\n",
    "<h4>{<a href=\"mailto:celee@nycu.edu.tw\">celee</a><sup>1</sup>, \n",
    "<a href=\"mailto:chtien@nycu.edu.tw\">chtien</a><sup>2</sup>}@nycu.edu.tw</h4>\n",
    "</center>\n",
    "\n",
    "<center><h3><a href=\"https://dop.nycu.edu.tw/ch/index.html\">Department of Photonics</a>, <a href=\"https://www.nycu.edu.tw/\">NYCU</a></h3></center>\n",
    "\n",
    "<br />\n",
    "<center><h5><a href=\"https://github.com/bruce88617/nycudopcs_advanced\">Data Science and Python Programming</a>, 2025 Spring</h5></center>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Last Time\n",
    "\n",
    "- Limit Theorems\n",
    "- Monte Carlo Method\n",
    "    * Area Estimation\n",
    "    * Univariate Integration\n",
    "    * Multi-variate Integration\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Today**\n",
    "\n",
    "<ul>\n",
    "    <li><a href=\"#tag1\">Point Estimation</a></li>\n",
    "    <li><a href=\"#tag2\">Maximum Likelihood Estimation</a></li>\n",
    "    <li><a href=\"#tag3\">Interval Estimation</a></li>\n",
    "    <li><a href=\"#tag4\">Hypothesis Testing</a></li>\n",
    "</ul>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Introduction**\n",
    "\n",
    "- In real life, we often work with data influenced by randomness, and we need to extract meaningful information and make conclusions based on that data. This randomness can arise from various sources. Here are two examples:\n",
    "\n",
    "    1. Suppose we want to analyze a baseball player's batting performance over a season. Since we cannot observe every possible scenario in which they might face a pitcher, we rely on a sample of past at-bats to estimate their batting average. The randomness in this case comes from the selection of at-bats, as different pitchers, game conditions, and player performance fluctuations affect the outcomes. Additionally, a player's form may change over time due to factors like fatigue or injuries, introducing another layer of randomness.\n",
    "\n",
    "    2. Another example occurs when tracking the flight of a baseball after it is hit. While we can predict its trajectory using physics, external factors such as wind speed, air resistance, and spin can alter its path in unpredictable ways. Here, the randomness comes from environmental conditions affecting the ball's motion.\n",
    "\n",
    "- Situations like these are common in many fields. The study of how to handle and analyze such randomness falls under the domain of statistical inference.\n",
    "\n",
    "- **Statistical inference** encompasses a set of techniques used to derive conclusions from data influenced by random variation.\n",
    "\n",
    "- Clearly, we use our knowledge of probability theory when we work on statistical inference problems. However, the biggest difference here is that we need to work with **real data**. For example, \n",
    "\n",
    "    > Let $X$ represent a candidate's approval rating in a poll (expressed as a percentage), assuming $X$ follows a normal distribution with mean $\\mu = 50$ and variance $\\sigma^2 = 25$. Find the probability that the candidate's approval rating exceeds 60%, i.e., $P(X > 60)$.\n",
    "\n",
    "- In real life, we might not know the distribution of $X$, so we need to collect data, and from the data we should be able to estimate $\\mu$ and $\\sigma$, and conclude whether $X$ follows a normal distribution or not.\n",
    "\n",
    "### Random Sampling\n",
    "\n",
    "- When collecting data, we often make several measurements (observations) on a random variable, for example, investigating the height distribution of adult males in Taiwan. To do this, we define random variables $X_1, X_2, X_3, ..., X_n$ as follows: We choose a random sample of size $n$ with replacement from the population and let $X_i$ be the height of the $i^{th}$ chosen male.\n",
    "\n",
    "    1. We choose a male uniformly at random from the population and let $X_1$ be the height of that male. Here, every male in the population has the same chance of being chosen.\n",
    "    2. Again, we choose a male uniformly (and independently from the first male) at random from the population and let $X_2$ be the height of that male. As usual, every male in the population has the same chance of being chosen.\n",
    "    3. In general, $X_i$ is the height of the $i^{th}$ male that is chosen uniformly and independently from the population.\n",
    "\n",
    "- About **with and without** replacement: \n",
    "\n",
    "    We often do the sampling without replacement in practice, i.e., we do not allow one person to be chosen twice. However, if the population is large, then the probability of choosing one person twice is extremely low, and it can be shown that the results obtained from sampling with replacement are very close to the results obtained using sampling without replacement. The big advantage of sampling with replacement (the above procedure) is that $X_i$'s will be independent and this makes the analysis much simpler.\n",
    "\n",
    "- If we would like to estimate the average height in the population, we may define an estimator as $$\\hat{\\Theta} = \\frac{X_1 + X_2 + X_3 + ... + X_n}{n},$$ which means that $\\hat{\\Theta}$ is a random variable because it depends on the random sample (variables) $X_1, X_2, X_3, ..., X_n$.\n",
    "\n",
    "- The collection of random variables $X_1, X_2, X_3, ..., X_n$ is said to be a **random sample** of size $n$ if they are **independent and identically distributed (i.i.d.)**.\n",
    "\n",
    "    1. $X_1, X_2, X_3, ..., X_n$ are independent random variables.\n",
    "    2. They have the same distribution, $$F_{X_1}(x) = F_{X_2}(x) = ... = F_{X_n}(x) \\text{, for all } x \\in \\Reals.$$\n",
    "\n",
    "- In the above example, the random variable $\\hat{\\Theta} = \\frac{X_1 + X_2 + X_3 + ... + X_n}{n}$ is called a **point estimator** for the average height of males in the population, and we can obtain $\\hat{\\Theta} = \\hat{\\theta}$. Here, $\\hat{\\theta}$ is called an **estimate** of the average height of males in the population. In general, a point estimator is a function of the random sample $\\hat{\\Theta} = h(X_1, X_2, X_3, ..., X_n)$ that is used to estimate an unknown quantity.\n",
    "\n",
    "#### Properties of Random Samples:\n",
    "\n",
    "- Let $X_1, X_2, X_3, ..., X_n$ be a random sample from the same distribution, we assume:\n",
    "\n",
    "    1. The $X_i$'s are independent.\n",
    "    2. $F_{X_1}(x) = F_{X_2}(x) = ... = F_{X_n}(x)$ for all $x \\in \\Reals$.\n",
    "    3. $\\mathbb{E}[X_i] = \\mathbb{E}[X] = \\mu < \\infty$.\n",
    "    4. $0 < \\text{Var}(X_i) = \\text{Var}(X) = \\sigma^2 < \\infty$.\n",
    "\n",
    "- Sample mean: $$\\bar{X} = \\frac{X_1 + X_2 + X_3 + ... + X_n}{n}.$$ Recall that:\n",
    "\n",
    "    1. $\\mathbb{E}[\\bar{X}] = \\mu$.\n",
    "    2. $\\text{Var}(\\bar{X}) = \\frac{\\text{Var}(X)}{n} = \\frac{\\sigma^2}{n}$\n",
    "    3. Weak law of large numbers (WLLN): $$\\lim_{n \\to \\infty} P(|\\bar{X} - \\mu| \\leq \\epsilon = 0).$$\n",
    "    4. Central limit theorem: the random variable $$Z_n = \\frac{\\bar{X} - \\mu}{\\frac{\\sigma}{\\sqrt{n}}} = \\frac{X_1 + X_2 + X_3 + ... + X_n - n \\mu}{\\sigma \\sqrt{n}}$$ converges to the standard normal random variable $N(0,1)$ as $n \\to \\infty$.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"tag1\"></a>\n",
    "\n",
    "## **Point Estimation**\n",
    "\n",
    "- Assuming $\\theta$ is an unknown parameter to be estimated, for example, the expected value of a random variable $\\theta = \\mathbb{E}[X]$. An important assumption here is that $\\theta$ is a deterministic (non-random) quantity. To estimate $\\theta$, we can define a point estimator $\\hat{\\Theta}$ that is a function of the random sample, i.e., $$\\hat{\\Theta} = h(X_1, X_2, X_3, ..., X_n).$$\n",
    "\n",
    "### Evaluation of Estimators\n",
    "\n",
    "- Before we start to introduce how to use point estimator for the estimation of a random variable, we need some quantities to evaluate the performance of estimators (i.e., how good they are?).\n",
    "\n",
    "#### 1. Bias\n",
    "\n",
    "- The bias of an estimator $\\hat{\\Theta}$, $B(\\hat{\\Theta})$, is how far $\\hat{\\Theta}$ on average is from the real value of $\\theta$.\n",
    "\n",
    "$$\n",
    "B(\\hat{\\Theta}) = \\mathbb{E}[\\hat{\\Theta}] - \\theta\n",
    "$$\n",
    "\n",
    "- If $B(\\hat{\\Theta}) = 0$, we say that $\\hat{\\Theta}$ is an **unbiased estimator** of $\\theta$.\n",
    "\n",
    "- - -\n",
    "##### Example: Bias of Sample Mean\n",
    "\n",
    "- Let $X_1, X_2, X_3, ..., X_n$ be a random sample. Then $\\hat{\\Theta} = \\bar{X} = \\frac{X_1 + X_2 + ... + X_n}{n}$ is an unbiased estimator of $\\theta = \\mathbb{E}[X_i]$.\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "B(\\hat{\\Theta}) &= \\mathbb{E}[\\hat{\\Theta}] - \\theta \\\\\n",
    "    &= \\mathbb{E}[\\bar{X}] - \\theta \\\\\n",
    "    &= \\frac{\\mathbb{E}[X_1] + \\mathbb{E}[X_2] + ... + \\mathbb{E}[X_n]}{n} - \\theta \\\\\n",
    "    &= \\frac{n \\cdot \\mathbb{E}[X_i]}{n} - \\theta\\\\\n",
    "    &= \\mathbb{E}[X_i] - \\theta \\\\\n",
    "    &= 0.\n",
    "\\end{aligned}\n",
    "$$\n",
    "- - -\n",
    "\n",
    "#### 2. Mean Squared Error (MSE)\n",
    "\n",
    "- From the above example, we can conclude that both $\\hat{\\Theta}_1 = \\bar{X}$ and $\\hat{\\Theta}_2 = X_i$ are unbiased estimator of $\\theta$. Nevertheless, $\\hat{\\Theta}_2$ is clearly not as good as $\\hat{\\Theta}_1$. Therefore, we need other measures to ensure that an estimator is a \"good\" estimator. A very common measure is the **mean squared error** defined by $$MSE(\\hat{\\Theta}) = \\mathbb{E} \\big[ (\\hat{\\Theta} - \\theta)^2 \\big].$$\n",
    "\n",
    "- Note that $\\hat{\\Theta} - \\theta$ is the error that we make when we estimate $\\theta$ by $\\hat{\\Theta}$. Thus, the MSE is a measure of the distance between $\\theta$ by $\\hat{\\Theta}$. A smaller MSE is generally indicative of a better estimator.\n",
    "\n",
    "- - -\n",
    "##### Example: MSE of Sample Mean and One Sample\n",
    "\n",
    "- Let $X_1, X_2, X_3, ..., X_n$ be a random sample from a distribution with mean $\\mathbb{E}[X_i] = \\theta$, and variance $\\text{Var}(X_i) = \\sigma^2$. Consider the following two estimators for $\\theta$:\n",
    "\n",
    "    1. $\\hat{\\Theta}_1 = \\bar{X} = \\frac{X_1 + X_2 + ... + X_n}{n}$.\n",
    "    2. $\\hat{\\Theta}_2 = X_i$.\n",
    "\n",
    "- If $n > 1$, we have $MSE(\\hat{\\Theta}_2) > MSE(\\hat{\\Theta}_1)$.\n",
    "\n",
    "- For $MSE(\\hat{\\Theta}_2)$:\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "MSE(\\hat{\\Theta}_2) &= \\mathbb{E} \\big[ (\\hat{\\Theta}_2 - \\theta)^2 \\big] \\\\\n",
    "    &= \\mathbb{E} \\big[ (X_i - \\mathbb{E} [X_i])^2 \\big] \\\\\n",
    "    &= \\text{Var}(X_i) \\\\\n",
    "    &= \\sigma^2.\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "- To find $MSE(\\hat{\\Theta}_1)$, we have\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "MSE(\\hat{\\Theta}_1) &= \\mathbb{E} \\big[ (\\hat{\\Theta}_1 - \\theta)^2 \\big] \\\\\n",
    "    &= \\mathbb{E} \\big[ (\\bar{X} - \\theta)^2 \\big] \\\\\n",
    "    &= \\text{Var}(\\bar{X} - \\theta) + \\big( \\mathbb{E}[\\bar{X} - \\theta] \\big)^2 \n",
    "        \\text{ } \\big( \\because \\mathbb{E}[Y^2] = \\text{Var}(Y) + (\\mathbb{E}[Y])^2 \\big) \\\\\n",
    "    &= \\text{Var}(\\bar{X}) \\\\\n",
    "    &= \\frac{\\sigma^2}{n}.\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "- Therefore, we conclude that:\n",
    "\n",
    "    1. $\\hat{\\Theta}_1 = \\bar{X}$ is probably a better estimator for the value of $\\theta$ (the expected value) since it has smaller MSE.\n",
    "    2. Let $\\hat{\\Theta}_1, \\hat{\\Theta}_2, ..., \\hat{\\Theta}_i, ...$ be a sequence of point estimators of $\\theta$. $MSE(\\hat{\\Theta}_i) \\to 0$ as $i \\to \\infty$.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "n = 10:\n",
      "    Bias = 0.0025961\n",
      "    MSE = 0.0000067\n",
      "n = 1:\n",
      "    Bias = 0.0185961\n",
      "    MSE = 0.0003458\n"
     ]
    }
   ],
   "source": [
    "from scripts.testFuncs import test1\n",
    "\n",
    "test1(n1=10, n2=1, numExp=100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- - -\n",
    "\n",
    "#### Brief Summary\n",
    "\n",
    "- $\\bar{X} = \\frac{X_1 + X_2 + ... + X_n}{n}$ is a better and reasonable point estimator for the mean $\\mathbb{E}[X_i] = \\theta$.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Point Estimators for Variance\n",
    "\n",
    "- To estimate the variance of a distribution $\\sigma^2 = \\mathbb{E} \\big[ (X - \\mu)^2 \\big]$, we have known that the variance itself is the mean of the random variable $Y = (X - \\mu)^2$. This suggests the following estimator for the variance\n",
    "\n",
    "$$\n",
    "\\hat{\\sigma}^2 = \\frac{1}{n} \\sum_{k=1}^{n} (X_k - \\mu)^2.\n",
    "$$\n",
    "\n",
    "- By linearity of expectation, $\\hat{\\sigma}^2$ is an unbiased estimator of $\\sigma^2$. However, in practice we often do not know the value of $\\mu$. Thus, **we have to replace $\\mu$ by our estimate of the $\\mu$ (i.e., the sample mean)**, to obtain the following estimator for $\\sigma^2$:\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "\\bar{S}^2 &= \\frac{1}{n} \\sum_{k=1}^{n} (X_k - \\bar{X})^2 \\\\\n",
    "    &= \\frac{1}{n} \\Bigg( \\sum_{k=1}^{n} X_k^2 - n \\bar{X}^2 \\Bigg).\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "- Intuitively, we are curious about what is the bias of our estimator of $\\sigma^2$, \n",
    "\n",
    "$$\n",
    "B(\\bar{S}^2) = \\mathbb{E}[\\bar{S}^2] - \\sigma^2\n",
    "$$\n",
    "\n",
    "- Let $X_1, X_2, X_3, ..., X_n$ be a random sample from a distribution with mean $\\mathbb{E}[X_i] = \\theta$, and variance $\\text{Var}(X_i) = \\sigma^2$. We have\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "\n",
    "\\because \n",
    "\\mathbb{E}[\\bar{X}^2] &= (\\mathbb{E}[\\bar{X}])^2 + \\text{Var}(\\bar{X}) \\\\\n",
    "    &= \\mu^2 + \\frac{\\sigma^2}{n}. \\\\\n",
    "\n",
    "\\implies \n",
    "\\mathbb{E}[\\bar{S}^2] &= \\frac{1}{n} \\Bigg( \\sum_{k=1}^{n} \\mathbb{E}[X_k^2] - n \\mathbb{E}[\\bar{X}^2] \\Bigg) \\\\\n",
    "    &= \\frac{1}{n} \\Bigg( \\sum_{k=1}^{n} \\Big( \\mu^2 + \\sigma^2 \\Big) - n \\Big( \\mu^2 + \\frac{\\sigma^2}{n} \\Big) \\Bigg) \\\\\n",
    "    &= \\frac{1}{n} \\Bigg( n \\Big( \\mu^2 + \\sigma^2 \\Big) - n \\Big( \\mu^2 + \\frac{\\sigma^2}{n} \\Big) \\Bigg) \\\\\n",
    "    &= \\frac{n-1}{n} \\sigma^2. \\\\\n",
    "\n",
    "\\therefore \n",
    "B(\\bar{S}^2) &= \\mathbb{E}[\\bar{S}^2] - \\sigma^2 \\\\\n",
    "    &= - \\frac{\\sigma^2}{n}.\n",
    "\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "- We conclude that $\\bar{S}^2$ is a biased estimator of the variance. Nevertheless, note that if $n \\to \\infty$, then $B(\\bar{S}^2) \\to 0$. \n",
    "\n",
    "- Also, we can obtain an unbiased estimator of $\\sigma^2$ by multiplying $\\bar{S}^2$ by $\\frac{n}{n-1}$, as the **sample variance** $S$\n",
    "\n",
    "$$\n",
    "S^2 = \\frac{1}{n-1} \\sum_{k=1}^{n} (X_k - \\bar{X})^2 \n",
    "    = \\frac{1}{n-1} \\Bigg( \\sum_{k=1}^{n} X_k^2 - n \\bar{X}^2 \\Bigg).\n",
    "$$\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of Experiment = 2:\n",
      "    Theoretical value of Variance = 0.0833333\n",
      "    Result of biased estimator of variance = 1.8900000\n",
      "    Result of unbiased estimator of variance = 2.1000000\n",
      "Number of Experiment = 100:\n",
      "    Theoretical value of Variance = 0.0833333\n",
      "    Result of biased estimator of variance = 0.9600000\n",
      "    Result of unbiased estimator of variance = 1.0666667\n"
     ]
    }
   ],
   "source": [
    "from scripts.testFuncs import test2\n",
    "\n",
    "test2(p=0.5, numTrials=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"tag2\"></a>\n",
    "\n",
    "## **Maximum Likelihood Estimation**\n",
    "\n",
    "- So far, we have discussed estimating the mean and variance of a distribution. Now, we would like to introduce a systematic way of parameter estimation, called the **Maximum Likelihood Estimation (MLE)**. Before we start to introduce the idea behind MLE, let us first look at an example.\n",
    "\n",
    "- - -\n",
    "\n",
    "### A Bag with 3 Balls\n",
    "\n",
    "- We have a bag that contains $3$ balls. Each ball is either red or blue, but we have no information in addition to this. Thus, the number of blue balls $\\theta$ might be 0, 1, 2, or 3. You are allowed to choose 4 balls at random from the bag with replacement. We define the random variables $X_1$, $X_2$, $X_3$, and $X_4$ as follows \n",
    "\n",
    "$$\n",
    "X_i = \\begin{cases} \n",
    "    1 &\\text{if the } i^{\\text{th}} \\text{ chosen ball is blue} \\\\ \n",
    "    0 &\\text{if the } i^{\\text{th}} \\text{ chosen ball is red} \n",
    "\\end{cases}.\n",
    "$$\n",
    "\n",
    "- Note that $X_i$'s are i.i.d. and $X_i \\sim Bernoulli(\\frac{\\theta}{3})$. After doing my experiment, we observe the following values for $X_i$'s. $$(x_1, x_2, x_3, x_4) = (1, 0, 1, 1).$$\n",
    "\n",
    "    1. For each possible value of $\\theta$, find the probability of the observed sample, $(x_1, x_2, x_3, x_4) = (1, 0, 1, 1)$.\n",
    "    2. For which value of $\\theta$ is the probability of the observed sample is the largest?\n",
    "\n",
    "- Since $X_i \\sim Bernoulli(\\frac{\\theta}{3})$, we have\n",
    "\n",
    "$$\n",
    "P_{X_i}(x) = \\begin{cases}\n",
    "    \\frac{\\theta}{3} &\\text{for } x = 1 \\\\\n",
    "    1 - \\frac{\\theta}{3} &\\text{for } x = 0 \\\\\n",
    "\\end{cases}.\n",
    "$$\n",
    "\n",
    "- Therefore, the joint PMF of $X_1$, $X_2$, $X_3$, and $X_4$ can be written as\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "P_{X_1, X_2, X_3, X_4}(x_1, x_2, x_3, x_4) &= P_{X_1}(x_1)P_{X_2}(x_2)P_{X_3}(x_3)P_{X_4}(x_4) \\\\\n",
    "\\implies P_{X_1, X_2, X_3, X_4}(1, 0, 1, 1) &= \\frac{\\theta}{3} \\cdot \\Big( 1 - \\frac{\\theta}{3} \\Big) \n",
    "\\cdot \\frac{\\theta}{3} \\cdot \\frac{\\theta}{3} \\\\\n",
    "    &= \\Big( \\frac{\\theta}{3} \\Big)^3 \\Big( 1 - \\frac{\\theta}{3} \\Big).\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "|$\\theta$|$P_{X_1, X_2, X_3, X_4}(1, 0, 1, 1; \\theta)$|\n",
    "|:------:|:------------------------------------------:|\n",
    "|0|0|\n",
    "|1|0.0247|\n",
    "|2|0.0988|\n",
    "|3|0|\n",
    "\n",
    "- From the table we see that the probability of the observed data is maximized for $\\theta = 2$. This means that the observed data is most likely to occur for $\\theta = 2$. For this reason, we may choose $\\hat{\\theta} = 2$ as our estimate of $\\theta$. This is called the **Maximum Likelihood Estimation (MLE)** of $\\theta$.\n",
    "\n",
    "- - -\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Definition of Maximum Likelihood Estimation (MLE)\n",
    "\n",
    "- Let $X_1, X_2, X_3, ..., X_n$ be a random sample from a distribution with a parameter $\\theta$. Suppose that we have observed $X_1 = x_1, X_2 = x_2, ..., X_n = x_n$.\n",
    "\n",
    "    1. If $X_i$'s are discrete, the the likelihood function is defined as $$L(x_1, x_2, ..., x_n; \\theta) = P_{X_1, X_2, ..., X_n}(x_1, x_2, ..., x_n; \\theta)$$.\n",
    "    2. If $X_i$'s are jointly continuous, the the likelihood function is defined as $$L(x_1, x_2, ..., x_n; \\theta) = f_{X_1, X_2, ..., X_n}(x_1, x_2, ..., x_n; \\theta)$$.\n",
    "\n",
    "- In some problems, it is easier to work with the **log likelihood function** given by\n",
    "\n",
    "$$\n",
    "\\ln L(x_1, x_2, ..., x_n; \\theta).\n",
    "$$\n",
    "\n",
    "- A **Maximum Likelihood Estimation (MLE)** of $\\theta$, shown by $\\hat{\\theta}_{ML}$ is a value of $\\theta$ that maximizes the likelihood function. Also, you should notice that the estimator $\\hat{\\Theta}_{ML} = \\hat{\\Theta}_{ML}(X_1, X_2, ..., X_n)$ is a random variable whose value when $X_1 = x_1, X_2 = x_2, ..., X_n = x_n$ is given by $\\hat{\\theta}_{ML}$.\n",
    "\n",
    "- - -\n",
    "\n",
    "#### Example (1 unknown parameter to estimate)\n",
    "\n",
    "- If $X_i \\sim Binomial(m, \\theta)$ and we have observed $X_1, X_2, X_3, ..., X_n$. Find the maximum likelihood estimator of $\\theta$.\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "L(x_1, x_2, ..., x_n; \\theta) \n",
    "    &= P_{X_1, X_2, ..., X_n}(x_1, x_2, ..., x_n; \\theta) \\\\\n",
    "    &= \\prod_{k=1}^{n} P_{X_k}(x_k; \\theta) \\\\\n",
    "    &= \\prod_{k=1}^{n} \\binom{m}{x_k} \\theta^{x_k} \\cdot (1 - \\theta)^{m - x_k} \\\\\n",
    "    &= \\Bigg[ \\prod_{k=1}^{n} \\binom{m}{x_k} \\Bigg] \\cdot \\theta^{\\sum_{k=1}^{n} x_k} \\cdot (1 - \\theta)^{\\sum_{k=1}^{n}(m - x_k)} \\\\\n",
    "    &= \\text{constant} \\cdot \\theta^{s} \\cdot (1 - \\theta)^{mn - s} \\text{, let } s = \\sum_{k=1}^{n} x_k\n",
    "\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "- Therefore, we have\n",
    "\n",
    "$$\n",
    "\\hat{\\theta}_{ML} = \\frac{1}{mn} \\sum_{k=1}^{n} x_k.\n",
    "$$\n",
    "\n",
    "- This suggests that the MLE can be written as\n",
    "\n",
    "$$\n",
    "\\hat{\\Theta}_{ML} = \\frac{1}{mn} \\sum_{k=1}^{n} X_k.\n",
    "$$\n",
    "\n",
    "- - -\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Exercise 4.1\n",
    "\n",
    "- If $X_i \\sim Exponential(\\theta)$ and we have observed $X_1, X_2, X_3, ..., X_n$. Find the maximum likelihood estimator of $\\theta$.\n",
    "\n",
    "- - -"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- In general, $\\theta$ could be a vector of parameters, and we can apply the same methodology to obtain the MLE. For example, if we have $k$ unknown parameters $\\theta_1, \\theta_2, ..., \\theta_k$, then we need to maximize the likelihood function $$L(x_1, x_2, ..., x_n; \\theta_1, \\theta_2, ..., \\theta_k)$$ to obtain the maximum likelihood estimators $\\hat{\\Theta}_1, \\hat{\\Theta}_2, ..., \\hat{\\Theta}_k$.\n",
    "\n",
    "#### Example (2 unknown parameters to estimate)\n",
    "\n",
    "- If $X_i \\sim N(\\theta_1, \\theta_2)$ and we have observed $X_1, X_2, X_3, ..., X_n$. $$f_{X_i}(x_i; \\theta_1, \\theta_2) = \\frac{1}{\\sqrt{2 \\pi \\theta_2}} e^{- \\frac{(x_i - \\theta_1)^2}{2 \\theta_2}}$$ Find the maximum likelihood estimators of $\\theta_1$ and $\\theta_2$.\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "L(x_1, x_2, ..., x_n; \\theta_1, \\theta_2) \n",
    "    &= \\prod_{k=1}^{n} f_{X_k}(x_k; \\theta_1, \\theta_2) \\\\\n",
    "    &= \\prod_{k=1}^{n} \\frac{1}{\\sqrt{2 \\pi \\theta_2}} e^{- \\frac{(x_i - \\theta_1)^2}{2 \\theta_2}} \\\\\n",
    "    &= (2 \\pi \\theta_2)^{- \\frac{n}{2}} \\cdot \\exp \\Bigg[ - \\sum_{k=1}^{n} \\frac{(x_i - \\theta_1)^2}{2 \\theta_2} \\Bigg] \\\\\n",
    "\n",
    "\\implies \\ln L(\\bold{x}; \\theta_1, \\theta_2) \n",
    "    &= - \\frac{n}{2} \\ln (2 \\pi) - \\frac{n}{2} \\ln \\theta_2 - \\frac{1}{2 \\theta_2} \\sum_{k=1}^{n} (x_i - \\theta_1)^2\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "- To find the maximum value, we have the derivatives with respect to $\\theta_1$ and $\\theta_2$,\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "\\frac{\\partial}{\\partial \\theta_1} \\ln L(\\bold{x}; \\theta_1, \\theta_2)\n",
    "    &= \\frac{1}{\\theta_2} \\sum_{k=1}^{n} (x_i - \\theta_1) = 0 \\\\\n",
    "\\frac{\\partial}{\\partial \\theta_2} \\ln L(\\bold{x}; \\theta_1, \\theta_2)\n",
    "    &= - \\frac{n}{2 \\theta_2} + \\frac{1}{2 \\theta_2^2} \\sum_{k=1}^{n} (x_i - \\theta_1)^2 = 0 \n",
    "\\end{aligned}.\n",
    "$$\n",
    "\n",
    "- By solving the above equations, we obtain the following maximum likelihood estimators for $\\theta_1$ and $\\theta_2$,\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "\\hat{\\theta}_1 &= \\frac{1}{n} \\sum_{k=1}^{n} x_i \\\\\n",
    "\\hat{\\theta}_2 &= \\frac{1}{n} \\sum_{k=1}^{n} (x_i - \\hat{\\theta}_1)^2\n",
    "\\end{aligned}.\n",
    "$$\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"tag3\"></a>\n",
    "\n",
    "## **Interval Estimation**\n",
    "\n",
    "- So far, we have discussed point estimation for $\\theta$. The point estimator $\\hat{\\theta}$ alone does not give much information about $\\theta$, which means we do not know how close $\\hat{\\theta}$ is to the real $\\theta$. Therefore, we will introduce the concept of **interval estimation**. In this approach, instead of giving just one value $\\hat{\\theta}$ as the estimation for $\\theta$, we will produce an interval that is likely to include the true value of $\\theta$. That means, instead of saying $$\\hat{\\theta} = 94.00,$$ we might report the interval $$[\\hat{\\theta}_l, \\hat{\\theta}_h] = [93.13, 94.87],$$ which we hope includes the real value of $\\theta$.\n",
    "\n",
    "- In interval estimation, there are two important concepts. \n",
    "\n",
    "    1. One is the length of the reported interval, $\\hat{\\theta}_h - \\hat{\\theta}_l$, which shows the precision with which we can estimate $\\theta$.\n",
    "    2. The second one is the confidence level that shows how confident we are about the interval. The confidence level is the probability that the interval that we construct includes the real value of $\\theta$.\n",
    "\n",
    "- - -\n",
    "\n",
    "### Definition of Interval Estimation\n",
    "\n",
    "- Let $X_1, X_2, X_3, ..., X_n$ be a random sample from a distribution with a parameter $\\theta$ that is to be estimated. An **interval estimator with confidence level** $1 - \\alpha$ consists of two estimators $\\hat{\\Theta}_l (X_1, X_2, X_3, ..., X_n)$ and $\\hat{\\Theta}_h (X_1, X_2, X_3, ..., X_n)$ such that $$P \\Big( \\hat{\\Theta}_l \\leq \\theta \\leq \\hat{\\Theta}_h \\Big) \\geq 1 - \\alpha,$$ for every possible value of $\\theta$. Equivalently, we say that $[\\hat{\\theta}_l, \\hat{\\theta}_h]$ is a $(1 - \\alpha) \\times 100 \\% $ **confidence interval** for $\\theta$.\n",
    "\n",
    "- - -\n",
    "\n",
    "#### Example of Interval Estimation\n",
    "\n",
    "- Let $X_1, X_2, X_3, ..., X_n$ be a random sample from a normal distribution $N(\\theta, 1)$. Find $95 \\%$ confidence interval for $\\theta$.\n",
    "\n",
    "- We start with a point estimator $\\hat{\\Theta}$ for $\\theta$. Because $\\theta$ is the mean of distribution, we can use the sample mean $$\\hat{\\Theta} = \\bar{X} = \\frac{X_1 + X_2 + ... + X_n}{n}.$$ Since $X_i \\sim N(\\theta, 1)$ and the $X_i$'s are independent, we conclude that $$\\bar{X} \\sim N(\\theta, \\frac{1}{n}).$$ By normalizing $\\bar{X}$, we can get a random variable $$Q = \\frac{\\bar{X} - \\theta}{\\frac{1}{\\sqrt{n}}} = \\sqrt{n} (\\bar{X} - \\theta)$$ has a $N(0, 1)$ distribution and **its distribution does NOT depend on the unknown parameter $\\theta$**. Therefore, we have $$P \\Big( -1.96 \\leq \\sqrt{n} (\\bar{X} - \\theta) \\leq 1.96 \\Big) = 0.95,$$ which is equivalent to $$P \\Big( \\bar{X} - \\frac{1.96}{\\sqrt{n}} \\leq \\theta \\leq \\bar{X} + \\frac{1.96}{\\sqrt{n}} \\Big) = 0.95.$$\n",
    "\n",
    "- Therefore, we can report the interval $$[\\hat{\\theta}_l, \\hat{\\theta}_h] = \\Big[ \\bar{X} - \\frac{1.96}{\\sqrt{n}}, \\bar{X} + \\frac{1.96}{\\sqrt{n}} \\Big]$$ as our $95 \\%$ confidence interval for $\\theta$.\n",
    "\n",
    "- - -\n",
    "\n",
    "### Pivotal Quantity (Pivot)\n",
    "\n",
    "- Let $X_1, X_2, X_3, ..., X_n$ be a random sample from a distribution with a parameter $\\theta$ that is to be estimated. The random variable $Q$ is said to be a **pivot** or a **pivotal quantity**, if it has the following properties:\n",
    "\n",
    "    1. It is a function of the observed data $X_1, X_2, X_3, ..., X_n$ and the unknown parameter $\\theta$, but it does not depend on any other unknown parameters: $$Q = Q(X_1, X_2, X_3, ..., X_n, \\theta).$$\n",
    "    2. The probability distribution of $Q$ does not depend on $\\theta$ or any other unknown parameters (such as $Q = \\sqrt{n} (\\bar{X} - \\theta) \\sim N(0, 1)$ in the above example).\n",
    "\n",
    "- - -\n",
    "\n",
    "### Summary of Pivotal Method for Finding Confidence Interval\n",
    "\n",
    "1. Find a pivotal quantity $Q(X_1, X_2, X_3, ..., X_n, \\theta)$.\n",
    "\n",
    "2. Find an interval $[q_l, q_h]$ for $Q$ such that \n",
    "\n",
    "$$\n",
    "P(q_l \\leq Q \\leq q_h) = 1 - \\alpha.\n",
    "$$\n",
    "\n",
    "3. Using algebraic manipulations, convert the above equation to an equation of the form\n",
    "\n",
    "$$\n",
    "P(\\hat{\\Theta}_l \\leq \\theta \\leq \\hat{\\Theta}_h) = 1 - \\alpha.\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- - -\n",
    "\n",
    "#### Example of Pivotal Method\n",
    "\n",
    "- Let $X_1, X_2, X_3, ..., X_n$ be a random sample from a normal distribution with known variance $\\text{Var}(X_i) = \\sigma^2$, and unknown mean $\\mathbb{E}[X_i] = \\theta$. Find a $1 - \\alpha$ confidence interval for $\\theta$. Assume that $n$ is large.\n",
    "\n",
    "- $\\because \\theta = \\mathbb{E}[X_i]$, a natural choice of finding the confidence interval for $\\theta$ is using the sample mean $$\\bar{X} = \\frac{X_1 + X_2 + ... + X_n}{n}.$$ $\\because n \\to \\infty$, we can conclude that $$Q = \\frac{\\bar{X} - \\theta}{\\frac{\\sigma}{\\sqrt{n}}} \\sim N(0,1).$$ In particular, $Q$ is a function of $X_i$'s and $\\theta$, and its distribution does not depend on $\\theta$ or any other unknown parameters. Thus, a $(1 - \\alpha)$ interval for standard normal random variable $Q$ can be stated as \n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "& P \\Big( -z_{\\frac{\\alpha}{2}} \\leq Q \\leq z_{\\frac{\\alpha}{2}} \\Big) = 1 - \\alpha \\\\\n",
    "\\implies \n",
    "& P \\Bigg( -z_{\\frac{\\alpha}{2}} \\leq \\frac{\\bar{X} - \\theta}{\\frac{\\sigma}{\\sqrt{n}}} \\leq z_{\\frac{\\alpha}{2}} \\Bigg) = 1 - \\alpha \\\\\n",
    "\\implies\n",
    "& P \\Bigg( \\bar{X} - z_{\\frac{\\alpha}{2}} \\cdot \\frac{\\sigma}{\\sqrt{n}} \n",
    "    \\leq \\theta \n",
    "    \\leq \\bar{X} + z_{\\frac{\\alpha}{2}} \\cdot \\frac{\\sigma}{\\sqrt{n}} \\Bigg) = 1 - \\alpha \n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "- We conclude that $\\Big[ \\bar{X} - z_{\\frac{\\alpha}{2}} \\cdot \\frac{\\sigma}{\\sqrt{n}}, \\bar{X} + z_{\\frac{\\alpha}{2}} \\cdot \\frac{\\sigma}{\\sqrt{n}} \\Big]$ is a $(1 - \\alpha) \\times 100 \\% $ confidence interval for $\\theta$.\n",
    "\n",
    "- - -"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
